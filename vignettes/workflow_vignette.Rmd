---
title: "scRNASeq workflow - Vignette"
author: "Rebekka Wegmann"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
---
## Hardware and operating system requirements
This workflow was tested on Linux systems (RedHat Enterprise server release  and Ubuntu 16.04 LTS), but it should be compatible with any UNIX flavor.
In principle, there are no special hardware requirements. However, the amount of memory needed increases with the size of the dataset, and run time for some methods can be decreased by parallel computing. Therefore, for larger datasets (>3000 cells), we recommend running the analysis on a HPC platform.

## Run time
On this example dataset (400 cells), running the complete workflow took ~5-10min on a desktop computer (Ubuntu 16.04 LTS, 8x Intel Core i7-4790 CPU, 16Gb of RAM)

## Setting up your R environment

### Estimated installation time
First: There are a lot of dependencies in this workflow. Downloading and installing all of them will take time. How long exactly the installation will take depends on how many of the basic bioconductor packages you already have installed, and on your internet connection. For me, setting up the complete environment from scratch took around 4-6 hours.

### R and Bioconductor version
Some of the bioconductor packages used in this workflow changed drastically between different versions, so that newer versions are no longer compatible with tho old ones. Therfore, please make sure your **R version is 3.6.1** and your **Bioconductor version is 3.10**. To install a specific version of bioconductor, install `BiocManager`:
```{r, eval = F}
install.packages("BiocManager")
```
You can then install packages using `BiocManager::install("package", version="3.10")`

### General dependencies
Some packages depend on `Rcpp` and `RcppArmadillo`, which require relatively recent versions of C/C++ compilers. This should in general not be an issue, but keep it in mind if you stumble upon errors involving those packages or any other C/C++ compilation related errors.

Lastly, installing packages from github requires the devtools package:

```{r, eval = F}
install.packages("devtools")

# on Windows, you might need to run this in case devtools does not find Rtools 3.5 (only works for older versions of devtools though):
library(devtools)
assignInNamespace("version_info", c(devtools:::version_info, list("3.5" = list(version_min = "3.3.0", version_max = "99.99.99", path = "bin"))), "devtools")
find_rtools() # is TRUE now
```

### Required packages
In this section, all packages needed to run this vignette are listed in the order that they are loaded. Note that because sometimes, major changes happen between diffrent versions of a package, make sure you are installing the versions provided below.

#### Packages that are required throughout the analysis
```{r, eval=F}
# from CRAN
install.packages("Rtsne") #Rtsne v. 0.15
install.packages("ggplot2") # ggplot2 v. 3.2.1
install.packages("data.table") # data.table v. 1.12.8
install.packages("RColorBrewer") # RColorBrewer v. 1.1-2
install.packages("mvoutlier") # mvoutlier 2.0.9, required by some functions from scater

devtools::install_github("bwlewis/irlba") # irlba 2.3.3, optional. Make sure you use irlba > 2.3.1, older versions contain a bug that results in unreliable output!

# from Bioconductor
BiocManager::install("scater", version="3.10") # scater v. 1.14.0
BiocManager::install("scran", version="3.10") # scran v. 1.14.5
```

#### Genome annotation (human)
```{r, eval=F}
# ensembldb 2.4.1 and EnsDb.Hsapiens.v86 2.99.0
BiocManager::install(c("ensembldb","EnsDb.Hsapiens.v86"), version="3.10")
# org.Hs.eg.db v. 3.4.1
BiocManager::install("org.Hs.eg.db", version="3.10")
```

#### Genome annotation (mouse)
```{r, eval=F}
# ensembldb 2.4.1 and EnsDb.Mmusculus 2.99.0
biocLite(c("ensembldb","EnsDb.Mmusculus.v79"))
# org.Mm.eg.db v. 3.4.1
biocLite("org.Mm.eg.db")
```

#### Feature selection
```{r,eval = F}
BiocManager::install("M3Drop", version="3.10")
```

#### Clustering
```{r, eval = F}
install.packages("cluster") # cluster v. 2.0.6
install.packages("dendextend") # dendextend v. 1.5.2
install.packages("Ckmeans.1d.dp") # Ckmeans.1d.dp v. 4.2.1
install.packages("dbscan") # DBSCAN v. 1.1-1
install.packages(c("mclust","mvtnorm")) # mclust v. 5.4 and mvtnorm 1.0.6
install.packages("dynamicTreeCut") # dynamicTreeCut v. 1.63-1 

BiocManager::install("SC3", version="3.10") # SC3 v. 1.4.2
install.packages("MCL") #MCL 1.0
```

In addition, on UNIX, an external, faster version of MC can be used which can be found [here](https://micans.org/mcl/). To install MCL, go to the "License & Software" tab on teh website, and then click on the "Download a a ready-to-install tarball containing the MCL-edge software" link. Then run the following (assuming you downloaded mcl-14-137.tar.gz):

```{r, eval = F, engine = 'bash'}
tar xzf mcl-14-137.tar.gz
cd mcl-14-137
./configure --prefix=$HOME/local
make install
```

This will install MCL to ~/local/bin/mcl.

#### Differential expression analysis
```{r,eval = F}
BiocManager::install("limma", version="3.10") # limma v. 3.32.5
```

### Troubleshooting: Maximum number of DLLS reached
This is an error that is encountered when exceeding the maximum number of packages that can be loaded. Because some of the packages used in this workflow have a lot of dependencies, it is likely to encounter this error. There are some workarounds:

1) Set a higher limit of DLLS: Go to your home directory. Create or edit a file called .Renviron and paste the following into it: R_MAX_NUM_DLLS = 500

2) Restart R and only load the packages you really need. Since the workflow always saves intermediate results, this quick and dirty solution works quite well.

## A note on gene expression units
Before we start, a brief but important note on units: RNA-Seq is inherently a method for relative quantification of gene expression. While UMI counts can be interpreted as "number of transcripts per cell", this unit is arbitrary and differs between each experiment and platform. Normalizing counts yields "normalized transcripts per cell", or, if scaled to the total number of counts per cell, "normalized counts per million total counts (CPMs)". However, **NONE of these units are comparable across different samples and even less so across different platforms!**. If you want to compare gene expression between different samples, you can do so only after appropriate between-sample normalization.

## Terminology
* Features: The term "features" refers to the things that were measured. In the context of scRNASeq, "features" means "genes", or, more accurately, "transcripts".
* Feature controls: This is a set of features we use for quality control. Here, the feature controls correspond to mitochondrial genes.
* Counts: Counts are the number of unique molecular identifiers (UMIs) mapped to each transcript. 
* Expression: The term "expression" refers to gene expression level [arbitrary units!]. Usually, we use the log2-transformed normalized counts as a measure of gene expression.
* Dropout: This is an observation of a zero count for a feature. Dropouts can be either true zeros (i.e., the gene is really not expressed), or technical artifacts (i.e. a transcript was not captured or failed to amplify). 

## Setting up paths for loading and saving data

**Note: You have to change these so that they match your directories!**

This example assumes your working directory is the scRNASeq_workflow/vignettes directory, thus `wd` is the path to the scRNASeq_workflow directory.

```{r, message = FALSE}
rm(list=ls())
graphics.off()

wd = ".."

#Directory where input files are stored
input_dir = file.path(wd,"example_data")

#Directory where code is stored
code_dir = file.path(wd,"code")

#where to save the output data?
out_data_dir = file.path(wd,"example_output")
if(!dir.exists(out_data_dir)) {dir.create(out_data_dir)}

#where to save the produced plots?
plotdir = file.path(wd,"example_plots")
if(!dir.exists(plotdir)) {dir.create(plotdir)}

#use extrenal MCL? ONLY POSSIBLE ON UNIX
use_external_mcl=FALSE

#path to your MCL binary (only needed if use_external_mcl = TRUE)
#mcl_path = "~/local/bin/mcl"

set.seed(17) #to make tSNE plots reproducible

source(file.path(code_dir,"scRNASeq_pipeline_functions.R"))

# loading the libraries that are required throughout the analysis
library_calls()
```


## Preparing the data
For this example, we will use a subset of a publicly available dataset from 10X genomics. The original dataset contains ~4500 Peripheral Blood Mononuclear Cells (PBMCs). The raw data can be downloaded [here](http://cf.10xgenomics.com/samples/cell-exp/2.0.1/pbmc4k/pbmc4k_raw_gene_bc_matrices.tar.gz). To make this vignette run in a reasonable amount of time, the example dataset (example_data/pbmc_example_counts.txt) was created from the raw data by randomly sampling 400 cells.

In this first part, we will read the count matrix, annotate the cells and genes, and construct a SingleCellExperiment object that contains the count data as well as all the metadata:

```{r,message=T,eval=T}
# Read the dataset
counts = read.delim(file.path(input_dir,"pbmc_example_counts.txt"),sep="\t")

# if we have genes that are not expressed in any cell, discard them
keep_feature = !gene_filter_by_feature_count(counts,0)
counts = counts[keep_feature,]

# make a table of metadata (e.g. batch, cell type annotation,treatment,...)
# Here, we do not have any such information, so we just give each cell a name
# Note that the rownames of this table have to correspond to the column names
# of the count matrix.
cell_annotation = data.frame(cell_idx=paste0("pbmc_C",seq(dim(counts)[2])))
rownames(cell_annotation) = colnames(counts)

# get gene annotations from ensembldb
# optional: get gene descriptions from org.Hs.eg.db (slows down the process a lot!)
# NOTE: the output of get_gene_annotations is a data.table sorted by gene identifier.
#       This means the genes are no longer in the same order as in the count matrix!
gene_annotations = get_gene_annotations(rownames(counts),organism = "human",get_descriptions = F)

# convert this to feature metadata
feature_annotation = as.data.frame(gene_annotations)
rownames(feature_annotation) = gene_annotations$gene_id
feature_annotation = feature_annotation[rownames(counts),]

#construct SingleCellExperiment object
sce =SingleCellExperiment(assays = list(counts = as.matrix(counts), logcounts=log2(as.matrix(counts)+1)),
                          colData = cell_annotation,
                          rowData = feature_annotation)

```

Note that the `get_gene_annotations` function expects the gene identifiers to be ensembl IDs. Also, currently, only annotations for human and mouse genes are supported.

## Quality control
We will use scater to calculate various QC metrics. See the [package vignette](http://bioconductor.org/packages/3.10/bioc/vignettes/scater/inst/doc/overview.html) for details. Then, we use the cyclone function in scran to assign cell cycle phases. Since this takes fairly long for larger datasets, we save the SCESet afterwards so we can just load it and continue the analysis at some later time. 

```{r,eval=T}
#calculate QC metrics
sce = calculate_QC_metrics(sce)

# assign cell cycle phase (based on the method from scran)
# because PBMCs are post-mitotic, most cells should be assigned to G0/G1 phase
cc = annotate_cell_cycle(sce)
sce$cell_cycle_phase = cc$phases
sce$cc_scores = cc$scores

#save the SCESet
save(sce,file=file.path(out_data_dir,"sce_raw.RData"))
```

### Visualizing gene-level QC metrics

We can use scater to plot various QC metrics. All of these plots are returned as ggplot objects, so it's very easy to modify them (e.g. add titles, change axis labels, ...). Saving plots is most convenient using `ggsave`. 

For example, we can visualize the relative expression (percentage of total counts) of the top 50 expressed genes in each cell:

```{r}
load(file.path(out_data_dir,"sce_raw.RData"))

p1 = plotHighestExprs(sce,feature_names_to_plot = "symbol", exprs_values = "counts") 
print(p1)

```

As expected, we mostly find ribosomal proteins and core metabolic genes among the top 50 genes.

By setting as_percentage to false, we can also make a similar plot for the absolute raw expression values per gene (I made a custom version for this, because the one in scater has a bug that make it show the sum of counts instead of mean):

```{r}
p1.2 = custom_plotHighestExprs(sce,feature_names_to_plot = "symbol",as_percentage=F,exprs_values = "counts", colour_cells_by=c("detected"))
p1.2 = p1.2 + xlab("Expression [raw counts]")
print(p1.2)

# to save a plot, use the ggsave function:
# ggsave(p1.2, file = "saved_example_plot.pdf",height=7,width=7)
```

In general, this picks up similar genes than the previous plot. Exceptions are genes that are expressed extremely high, but only in few cells, where the ranking according to percentage of total counts is much lower than the one according to mean expression. 

Next, we can plot the detection rate v.s. mean expression for each gene. This gives a general idea of the quality of our data, i.e. how many genes were detected, what was the dropout rate etc.

```{r,warning=F}
p2 = plotRowData(sce, x="mean", y="detected") 
p2 = p2+xlab("Mean expression [raw counts]")+ggtitle("Mean expression versus detection rate") + scale_x_log10()
print(p2)
# Check total number of zeroes
t = table(counts(sce)==0)
print(t/sum(t)*100)
```

From these three plots, we can see that the data are very sparse. We find a high number of dropouts and even for the most highly expressed genes, there are still some cells having zero counts. Moreover, only very few genes are expressed in more than 50% of all cells.
If we check for the total number of zero counts, we see that over 90% of all counts are zeroes, which indicates that probably sequencing depth was not very high for this dataset or the cells had rather low RNA content.

### Identifying confounders

Scater also includes some functions that help to identify possible confounding factors:

```{r}
vars = c("sum","detected","cell_cycle_phase")
v = getVarianceExplained(sce, variables = vars)
p4 = plotExplanatoryVariables(sce, variables=vars)
print(p4 + ggtitle("Percentage of explained variance"))
```

### Filtering low-quality cells

As a first step, we set manual thresholds for the following QC metrics:

* the total number of detected features (min_genes, on log2 scale)
* the total number of UMIs ("counts") per cell (min_UMI, on log2 scale)
* the percentage of counts attributed to mitochondrial genes (mt_threshold)

```{r}
min_genes = 9.0 #minimum number of features (genes) per cell [log2]
min_UMI = 10.5  #minimum total UMIs / cell [log2]
mt_threshold = 9 #Maximum percentage of mitochondrial genes
```

We can check whether those thresholds make sense by calling the `plot_RNA_QC` and `plot_MT_QC` functions. For the RNA content, we would like to get rid of the left-hand tails of the distributions. For the mitochondrial genes, we remove outliers. On the second plot of the MT QC, we can see that most cells with very high mitochondrial gene content have overall poor coverage. 

```{r}
plot_RNA_QC(sce, min_genes = min_genes, min_UMI = min_UMI)
plot_MT_QC(sce,mt_threshold)

```

Now that we are happy with the thresholding, we can filter the cells:

```{r}
sce$keep_manual = (   !cell_filter_by_feature_count(counts(sce),2^min_genes) &
                      !cell_filter_by_total_UMI(counts(sce),2^min_UMI) &                                !cell_filter_by_mt_content(sce$subsets_MT_percent,mt_threshold))

table(sce$keep_manual)

sce_clean = sce[,sce$keep_manual]
```

Note that all filtering functions return logical vectors. It is therefore also possible to just flag the cells as outliers, but keep them in the analysis.

Instead of manual thresholding, we can use the `quickPerCellQC` funstion fram scater:

```{r}
qc_stats = quickPerCellQC(sce, percent_subsets="subsets_MT_percent")

table(qc_stats$discard, !sce$keep_manual)
```
Here, this removes mostly the same cells as the manual thresholds.

Next, we filter the genes. There are two parameters to set:

* n_th: The minimum number of cells
* min_counts: The minimum number of counts

The filter will then remove any gene not having at least `min_counts` counts in at least `n_th` cells. 

Here, we want to remove all genes that do not have at least 2 counts in at least 1 cell.

```{r}
n_th = 1
min_counts = 2

keep_feature = !(gene_filter_by_feature_count(counts(sce_clean),n_th, min_counts))
sce_clean = sce_clean[keep_feature,]

sce_clean = calculate_QC_metrics(sce_clean,subset = list(MT=which(rowData(sce_clean)$chr=="MT")))
save(sce_clean, file = file.path(out_data_dir,"sce_clean.RData"))
```

To visualize the raw data, we can make a PCA plot using `my_plot_PCA`. For a detailed description of all parameters of this function, check the [function documentation](#miscellaneous-plotting). For very large datasets, it is a good idea to set `use_irlba=TRUE`. This will then use `prcomp_irlba` to perform an approximate calculation of the first couple of principal components, which is much faster than PCA.

Because the counts are spread over several orders of magnitude, it is generally a good idea to log-transform them before running PCA:

```{r}
p = my_plot_PCA(counts = log2(counts(sce_clean)+1),
                scale_pca = T, center_pca = T, return_pca = F, use_irlba=F,
                color = colData(sce_clean)[,"sum",drop=F])
p = p+ggtitle("PCA on raw log2(counts)")
print(p)
```

Coloring the PCA by the total number of features detected shows that the first principal component primarily separates cells by the total number of features detected. This is expected for raw data and we will see this trend disappear after normalization.

Alternatively, we can make a tSNE projection of the log2-transformed counts using `my_plot_tSNE`:

```{r}
p = my_plot_tSNE(counts = log2(counts(sce_clean)+1),
                 is_distance = F, scale_pca = F, n_comp = 50, return_tsne=F,
                 color = colData(sce_clean)[,"sum",drop=F])
p = p+ggtitle("t-SNE on raw log2(counts)")
print(p)
```

Here, we also see some separation according to the number of total detected features, but less strongly than in the PCA. In addition, we start to see 3 distinct clusters. This is what we would expect as there are 3 most abundant cell types among PBMCs (B-cells, T-cells/NK cells and monocytes).

## Normalization
Normalizing the data is required to remove systematic differences between cells that arise from different RNA capture efficiency, sequencing depth, or differences in RNA content between cells. The function `normalize_counts` adds the normalized expression data (on a log2 scale) to the `norm_exprs` slot of the SCESet. We can choose between different normalization methods (see function documentation). In general, they perform very similar to each other. However, because we have very sparse data and methods originally developed for bulk RNA-Seq data (TMM, RLE, TC) often have issues handling many zeroes, we use the method implemented in the scran package which was specifically designed to work on sparse data.

```{r,message=F}
# normalize data
sce_clean = normalize_counts(sce_clean,method = "scran")

# normalized values are automatically log2-transformed and
# stored in the norm_exprs slot of the SCESet
norm_exprs(sce_clean)[1:5,1:5]

save(sce_clean, file = file.path(out_data_dir,"sce_clean.RData"))
```

We can again visualize the data using PCA and tSNE:

```{r}
# PCA of normalized values
sum_norm = data.frame(sum_expression = colSums(norm_exprs(sce_clean)))
p1 = my_plot_PCA(counts = norm_exprs(sce_clean),
                 color=sum_norm,
                 return_pca = F, scale_pca = T, center_pca = T)
p1 = p1+ggtitle("PCA on normalized counts")
print(p1)

#tSNE of normalized values
p2 = my_plot_tSNE(counts = norm_exprs(sce_clean),
                  color=sum_norm,
                  return_tsne = F, is_distance = F)
p2 = p2 + ggtitle("t-SNE (50 PCs) on normalized counts")
print(p2)
```

As expected, after normalization, the first principal component is no longer separating cells by how many genes they express and we now find 3 distinct clusters, similar to what we saw in the initial tSNE projection.

## Feature selection 
Feature selection is a crucial part of this workflow, because by selecting the most informative genes, we can greatly improve the signal/noise ratio. The workflow includes multiple ways of selecting interesting genes:

1. Using prior knowledge: select all the genes that are annotated with some GO term of interest. Here, we will use GO:0002376 (immune system process) as an example.
2. Based on sample variance: This method is described by [Brennecke et. al, 2013](http://www.nature.com/nmeth/journal/v10/n11/full/nmeth.2645.html?foxtrotcallback=true). Informative genes are those with higher than expected sample variance.
3. Based on a mean-dispersion fit using a depth-adjusted negative binomial model (DANB): This method is implemented in versions > 2.0 of M3Drop (Andrews and Hemberg, pre-print can be found [here](https://www.biorxiv.org/content/early/2016/10/20/065094))

In the following, we will use the different approaches and see how they affect our PCA plot. To make sure we actually see true cell type clusters, we color the PCA by the expression of a known B-cell marker (ENSG00000156738, i.e. CD20). 

For a detailed description of each function used in this section, check the [Function documentation](#feature-selection-2)

### Feature selection based on GO annotation

```{r,message=F}
cd20 = t(norm_exprs(sce_clean["ENSG00000156738",]))
colnames(cd20) = "CD20"

go_id = "GO:0002376" 
ens_go = GO_to_gene(go_id)
info_GO = rownames(sce_clean)%in%ens_go
table(info_GO)

p = my_plot_PCA(counts = norm_exprs(sce_clean[info_GO,]),
                 return_pca = F, scale_pca = T, center_pca = T,
                title = "PCA - GO:0002376 features",
                color = cd20)
print(p)
```

### Feature selection using highly-variable genes method

```{r}
info_HVG = info.genes(2^norm_exprs(sce_clean)-1,PLOT=T,qcv=0.25,pv=.1,q=.5,minBiolDisp = 0) 
table(info_HVG)
p = my_plot_PCA(counts = norm_exprs(sce_clean[info_HVG,]),
                 return_pca = F, scale_pca = T, center_pca = T,
                title = "PCA - HVG features",
                color = cd20)
print(p)
```

### Feature selection using DANB
The `run_DANB` function is a wrapper to both NBDrop and NBDisp methods that are implemented in newer versions (>2.0) of the M3Drop package. Briefly, NBDrop selects genes with unexpected dropout rates while NBDisp selects informative genes based on higher than expected dispersions. In general, NBDrop is less prone to selecting very low expressed genes, but on the downside, if the the total number of features detected per cell is a confounder, NBDrop can enhance this effect. Note that DANB requires raw counts as input.

To modify the number of genes selected, you can either set the `cutoff` variable, which is a p-value cutoff for NBDrop, or set `perc_genes`, in which case the top `perc_genes` percentage of genes will be chosen.

```{r}
info_NBdrop = run_DANB(counts(sce_clean),method = "NBDrop",save_plot=F, perc_genes=10) 
info_NBdisp = run_DANB(counts(sce_clean),method = "NBDisp",save_plot=F, perc_genes = 10) 
table(info_NBdrop,info_NBdisp)

p = my_plot_PCA(counts = norm_exprs(sce_clean[info_NBdrop,]),
                 return_pca = F, scale_pca = T, center_pca = T,
                title = "PCA - NBDrop features",
                color = cd20)
print(p)
p = my_plot_PCA(counts = norm_exprs(sce_clean[info_NBdisp,]),
                 return_pca = F, scale_pca = T, center_pca = T,
                title = "PCA - NBDisp features",
                color = cd20)
print(p)
```

### Feature selection - Conclusion
In conclusion, we see that by selecting a biological process that we know affects the expected cell types, we can increase the variation between the three observed clusters while cells belonging to the same cluster move closer together. Also, note how the variation explained by the first two principal components increases drastically when we select a small number of informative genes. A similar result is obtained when using DANB with either the NBDisp or NBDrop method, with NBDrop in this case outperforming NBDisp. The HVG method fails to improve this signal/noise ratio. This is most likely because it was originally developed for fluidigm C1 data (no UMIs).  On our type of data, it tends to be biased towards low expressed genes as the underlying mean-dispersion model is not fitting well.

I suggest to exclusively use DANB in the future. Which version of this method is best depends on your data:

* The NBDrop version  works very well if you expect your highly variable genes to have a binary expression pattern (expressed in one cell type but not expressed at all in others). If there are (technical or biological) differences in dropout rates between cell types, however, NBDrop selects genes that only separate cells according to dropout rate. 

* NBDisp selects genes based on dispersion, which is dependent on variance. NBDisp is therefore slightly more biased towards low-expressed genes. In general, this is not really a problem and I found NBDisp to perform very well on all datasets tested.

In the following, we will use only the genes selected by NBDrop.

```{r, eval=T}
sce_info = sce_clean[info_NBdrop,]
dim(sce_info)

# tSNE map of the cleaned data
# note that by setting return_tsne = T, we can obtain the t-SNE object for later use
set.seed(17) #set seed again, some packages internally set a different one
tsne_info = my_plot_tSNE(counts = norm_exprs(sce_info),
                         scale_pca = F, n_comp = 50, return_tsne=T)$tsne
```

We will save the SCESet and the t-SNE map for later use:

```{r,eval=T}
save(sce_info, file = file.path(out_data_dir,"sce_info.RData"))
save(tsne_info,file = file.path(out_data_dir,"tsne_info.RData"))
```

```{r}
#load the data we need
load(file.path(out_data_dir,"sce_info.RData"))
load(file.path(out_data_dir,"tsne_info.RData"))
```

## Clustering
The workflow contains different types of clustering approaches. Which one is most appropriate depends on your data. As a rule of thumb, if your data contains clearly separated clusters, all algorithms should produce a similar result. If you get very different results, maybe there really are no clear clusters and you might be better off using an approach tailored to more continuous cell type changes (e.g. monocle, SLICER). A good overview of the different decisions required when carrying out cluster analysis can be found in [this](https://arxiv.org/pdf/1503.02059.pdf) paper by Henning et. al.

For a detailed description of the functions used in this section, see [Function documentation](#clustering-2)

Before we try any of the methods, we do a quick and dirty  manual annotation of the four major cell types based on marker expression. We use CD20 expression to identify B-cells, the sum of CD14, LYZ, FCGR3A and MS4A7 for any type of Monocyte, CD3 for T-cells and the sum of GNLY and NKG7 for natural killer cells.

```{r, fig.align='center', fig.width=12, fig.height=8}
b_cell = t(norm_exprs(sce_info["ENSG00000156738",]))
colnames(b_cell) = "B-cell"

monocyte = data.frame(Monocyte = colSums(norm_exprs(sce_info)[which(rowData(sce_info)$symbol %in% c('CD14','LYZ','FCGR3A','MS4A7')),]))

t_cell = data.frame(`T-cell` = colSums(norm_exprs(sce_info)[which(rowData(sce_info)$symbol %in% c('CD3E','CD3D','CD3G')),]))

nk_cell = data.frame(`NK cell` = colSums(norm_exprs(sce_info)[which(rowData(sce_info)$symbol %in% c('GNLY','NKG7')),]))

# Make plots
# Note that by providing the tsne input variable instead of counts,
# we can use an existing t-SNE calculation for plotting

p1 = my_plot_tSNE(tsne = tsne_info, color = b_cell, title = "B-cell marker expression")  
p2 = my_plot_tSNE(tsne = tsne_info, color = monocyte, title = "Monocyte marker expression")
p3 = my_plot_tSNE(tsne = tsne_info, color = t_cell, title = "T-cell marker expression")
p4 = my_plot_tSNE(tsne = tsne_info, color = nk_cell, title = " NK cell marker expression")
ggmultiplot(p1,p2,p3,p4,cols=2)
```

```{r}
assignment = data.table(tsne1 = tsne_info$Y[,1], tsne2 = tsne_info$Y[,2],cell_type = 'T-cell')
assignment[tsne1 < -10 ,cell_type:='Monocyte']
assignment[tsne2 < -10 ,cell_type:='B-cell']
assignment[tsne2 > 15 & tsne1 > 5,cell_type:='NK Cell']

sce_info$cell_type = assignment$cell_type

p = my_plot_tSNE(tsne = tsne_info, color = colData(sce_info)[,"cell_type",drop=F])
print(p+labs(title="t-SNE on informative genes",subtitle = "Colored by manual cell annotation"))

```

### Single-cell consensus clustering (SC3)
SC3 ([Kiselev et. al, 2016](http://www.nature.com/nmeth/journal/v14/n5/full/nmeth.4236.html?foxtrotcallback=true)) is a tool for unsupervised clustering that integrates multiple clustering approaches and constructs a consensus clustering from them. In a nutshell, different distance measures and transformations are considered and for each combination, a k-means clustering is run. From this, a consensus matrix is constructed that summarizes how often each cell is in the same cluster with each other cell. The final clustering otput is obtained by hierarchical clustering of the consensus matrix.

The advantages of SC3 are that it is more robust to clustering inputs and parameter selection than other methods. Moreover, it was developed to work together with scater and therefore is very easy to use in combination with the rest of this workflow.

The downside of SC3 is that the many clusterings that are generated take some time. According to the authors, SC3 takes ~20 min to run on a dataset with 2000 cells. Another thing to keep in mind is that SC3 uses k-means clustering, which in turn assumes all clusters are equaly sized spheres. In cases where this assumption does not hold, k-means will either split big clusters in a lot of tiny ones or, if forced to use a smaller number of clusters, fail completely. 

To run SC3 with the default settings, we just need one function, `sc3()`. Here, we will manually run the individual steps of SC3 to make the analysis a bit more transparent. 

First, we prepare the SCESet for the analyisis. This will add all the relevant parameters to the sc3 slot of the object. The parameters we set are

* ks: the number of clusters we would like to test
* n_cores = number of cores used in parallel computing. NOTE: if you do not set this parameter, the function will use the number of available cores -1.

```{r, echo=F}
library(SC3)
```

```{r,eval=T}
library(SC3)
#newer versions of sc3 require that there is a feature_symbol attribute for each gene. Let's quick fix and put the symbol slot there:
rowData(sce_info)$feature_symbol = rowData(sce_info)$symbol 
sce_info = sc3_prepare(sce_info, n_cores = 4)
```

Next, we let SC3 determine the best number of clusters to use. For a detailed description on how the number of clusters is chosen, see the methods section of the [paper](http://www.nature.com/nmeth/journal/v14/n5/full/nmeth.4236.html?foxtrotcallback=true) 

```{r,eval=T}
sce_info = sc3_estimate_k(sce_info)
str(metadata(sce_info)$sc3)
```

And now we run SC3 using 5 clusters. Setting biology = TRUE tells SC3 that it should calculate biological properties of the clusters:

```{r,eval=T}
sce_info = sc3(sce_info, ks = 5, biology = TRUE, n_cores = 4)
```

Note: If your dataset contains more than 5000 cells, SC3 will automatically switch to the "hybrid SVM" version of the algorithm. In that case, skip the above and run the following instead:

```{r, eval = F}
sce_info = sc3(sce_info, ks = 6, biology = F, n_cores = 8)
sce_info = sc3_run_svm(sce_info)

sce_info@sc3$svm_train_inds = NULL
sce_info = sc3_calc_biology(sce_info, k=c(8,13), regime = "marker")

# to visualize the markers, use my modified fuction:

# change the plotted gene names to symbol for better readability
plot_sce = sce_info
rownames(plot_sce) = rowData(plot_sce)$symbol
custom_sc3_plot_markers(plot_sce, k=6, p.val = 0.01, auroc = 0.90)

rm(plot_sce)
```

SC3 also has a couple of nice visualization functions. For example, we can plot the consensus matrix:

```{r, out.width="110%",fig.height=4}
sc3_plot_consensus(sce_info, k=5)
```

In this plot, a value of 1 means the cells are always together in the same cluster. A value of 0 means they are never in the same cluster. Values in between indicate that it is not so clear where the cell belongs.

We can also make a heatmap of gene expression. With the show_pdata parameter, we can add any column in colData sce to annotate the cells:

```{r, out.width="110%",fig.height=4}
sc3_plot_expression(sce_info, k = 5, show_pdata = c("cell_type"))
```

Because we set biology to TRUE, SC3 determined marker genes for each cluster. We can have a look at them here (NOTE: use the custom version of the `sc3_plot_markers` function if you used the hybrid SVM approach!):

```{r, out.width="110%",fig.height=6}
# change the plotted gene names to symbol for better readability
plot_sce = sce_info[!is.na(rowData(sce_info)$feature_symbol),]
rownames(plot_sce) = rowData(plot_sce)$symbol

sc3_plot_markers(plot_sce, k=5, p.val = 0.1, auroc = 0.80, show_pdata = c("cell_type")) 

# for hybrid SVM approach:
# custom_sc3_plot_markers(plot_sce, k=5, p.val = 0.01, auroc = 0.90) #need to fix for new version
```

In this plot, we see that SC3 correctly identified known markers for the different cell types. In addition, we see that there are 2 larger groups among the T-cells. Checking the marker expression, we find that cluster 5 (high CCL5 and IL32) corresponds to CD8+ cytotoxic T-cells, whereas the remaining T-cells likely are T helper cells, although we did not find the characteristic CD4 marker. We can visualize our assignment on the t-SNE map. Setting `show_proprtions=T` adds the relative number of cells per cluster to the legend:

```{r}
assignment = data.table(clust = sce_info$sc3_5_clusters, cell_type = sce_info$cell_type)
assignment[clust == 1, cell_type:= 'CD4 T-cell']
assignment[clust == 2, cell_type:= 'Monocyte']
assignment[clust==3,cell_type:='B-cell']
assignment[clust==4,cell_type:='NK cell']
assignment[clust == 5, cell_type:= 'CD8 T-cell']

sce_info$SC3_assignment = assignment$cell_type
p = my_plot_tSNE(tsne = tsne_info,
                 color = colData(sce_info)[,"SC3_assignment",drop=F],
                 shape = colData(sce_info)[,"cell_type",drop=F],
                 title = "SC3 assignment",
                 show_proportions = T)
print(p)
save(sce_info,file = file.path(out_data_dir,"sce_info.RData"))
```

### Hierarchical clustering (hclust)
Hierarchical clustering groups items according to some measure of similarity by sequentially joining the two most similar observations into a cluster. Therefore, selecting an informative distance measure is crucial for this algorithm to work. We will consider two commonly used distance measures in the following.

#### Euclidean distance
Euclidean distances in very high dimensional space tend to get all very large and very similar to each other, making cluster identification near impossible. As a workaround, we will reduce the dimensionality of the dataset using PCA before calculating the cell-cell distances.

```{r}
pca = my_plot_PCA(counts = norm_exprs(sce_info),return_pca=T)$pca
screeplot(pca,type = 'lines')
```

From the screeplot, we see that after the 6th component, the variance stays more or less constant. We therefore select 6 components for the distance calculation:

```{r}
dist_eucl = dist.gen(pca$x[,1:6],method='euclidean')
hfit = hclust(dist_eucl,method="average")
plot(hfit, labels = F, main = "hclust on euclidean distance in PCA space")
```

In this dendrogram, we see that there seem to be 6 major clusters present, however, they are somewhat nested. A fixed height cut of the dendrogram would here merge some clusters that should not be merged while creating small sets of outlier cells. To prevent this, we can use dynamic ct of the dendrogram:

```{r, message =F}
library(dynamicTreeCut)
groups_hclust_eucl = cutreeDynamic(hfit, distM = as.matrix(dist_eucl), deepSplit = 0, minClusterSize = 5, maxCoreScatter = 0.70, minGap = 0.25)
```

To get an idea of the quality of our clustering, we can make a silhoutte plot:

```{r}
library(cluster)
si = silhouette(groups_hclust_eucl,dist_eucl)
plot(si, col = "darkgray", border=NA, main = "Silhouette plot for hclust (euclidean in PCA space)")
```

The cluster silhoutte is a measure of how similar a point is to other points in the same cluster, relative to how similar it is to points in the closest cluster it does not belong to. A value close to 1 means the point is very well clustered. Values close to 0 indicate points lying between clusters. A negative value means the point is probably misassigned.

We can also plot the silhoutte values on the tSNE map:
```{r}
sce_info$hclust_sil = si[,3]
p = my_plot_tSNE(tsne = tsne_info,
                 color = colData(sce_info)[,"hclust_sil",drop=F],
                 shape = colData(sce_info)[,"cell_type",drop=F],
                 title = "tSNE colored by silhouette width")
print(p+scale_color_distiller(type="div", palette = "RdBu"))
```

As expected, the points with low silhoutte values are in between the clearly visible clusters.

Now we can compare the new assignment with what we obtained from SC3:

```{r}
sce_info$hclust_eucl = as.factor(groups_hclust_eucl)
table(sce_info$SC3_assignment,sce_info$hclust_eucl)
```

We see that the assignments of the two methods are very similar. In addition to the clusters found by SC3, hclust identified two tiny clusters of monocytes (clusters 6 and 7). Let's have a look at the expression of some monocyte and dendritic cell marker genes. To do so, we make use of the `plotExpression` function from the scater package:

```{r}
# change the plotted gene names to symbol for better readability
plot_sce = sce_info
rownames(plot_sce) = rowData(plot_sce)$symbol
monocyte_markers = c('CD14','LYZ','FCGR3A','MS4A7','FCER1A','CST3')
p = plotExpression(plot_sce,features = monocyte_markers, x="hclust_eucl",
               colour_by = "hclust_eucl")
print(p)
```

We see that our small population of monocytes has low expression of LYZ and CD14 but high expression of CD16 (FCGR3A) and MS4A7. We therefore conclude that this small population corresponds to the CD14+/CD16++ monocytes whereas the remaining monocytes are the "classical" CD14++/CD16- monocytes.Cluster 7 is CD14/CD16 negative, and expresses high levels of FCER1A and CST3. We therefore label this cluster as dendritic cells.

We can now label our assignment accordingly and visualize on tSNE map:

```{r,fig.width = 8}
sce_info$hclust_eucl = factor(sce_info$hclust_eucl, levels = levels(sce_info$hclust_eucl), labels = c("CD4 T-cell","CD14++/CD16- Monocyte","B-cell","CD8 T-cell","NK cell","CD14+/CD16++ Monocyte", "Dendritic cell"))
p = my_plot_tSNE(tsne = tsne_info, color = colData(sce_info)[,"hclust_eucl",drop=F],
                 shape = colData(sce_info)[,"cell_type",drop=F],
                 title = "hclust (euclidean) assignment",
                 show_proportions = T)
print(p)
```

#### Pearson distance
Another possible distance measure is Pearson correlation, where the distance is calculated as 1-correlation. We again try to get 6 clusters:

```{r}
library(dendextend)
dist_pearson = dist.gen(t(norm_exprs(sce_info)),method = "pearson")
hfit = hclust(dist_pearson,method="average")
groups_hclust_pearson = cutree(hfit, k=6)
sce_info$hclust_pearson = as.factor(groups_hclust_pearson)
hfit2 = color_branches(hfit, k=6)
hfit2 = hfit2 %>% set("labels", rep("",dim(sce_info)[2]))
plot(hfit2, main = "hclust on Pearson correlation")
```

From the dendrogram, we can already guess that we did not get a very fine-grained clustering. But let's check on the tSNE-map anyway:

```{r,fig.width=8}
p = my_plot_tSNE(tsne = tsne_info, color = colData(sce_info)[,"hclust_pearson",drop=F],
                 shape = colData(sce_info)[,"cell_type",drop=F],
                 title = "hclust (Pearson) assignment",
                 show_proportions = T)
print(p)
```

While we can find the major cell types, we can no longer identify subgroups within the T-cells and the monocytes. Most likely this is due to the underlying noise in the data, which makes all correlations very similar. Sometimes, using the euclidean distance of the correlation matrix improves the clustering. We could also use PCA here to reduce the number of dimensions before calculating the correlation matrix. This would hopefully increase the signal to noise ratio, but since we already succesfully applied dimensionality reduction in combination with euclidean distance, we leave it be.

In conclusion, hierarchical clustering is a very useful clustering technique that does not require any assumptions on the number of clusters (at least, not before the algorithm is run, you still have to decide where to cut the dendrogram afterwards) or the distribution the data come from. However, selecting an informative input distance is crucial. Reducing dimensionality before calculating distances often improves the signal to noise ratio, but might come at the cost of loosing some information. The major drawback of hierarchical clustering is that it becomes infeasible for very large datasets (ten thousands of cells).

### Graph-based methods

#### MCL 
The MCL algorithm is short for the Markov Cluster Algorithm. It is an unsupervised cluster algorithm for graphs based on simulation of (stochastic) flow in graphs ([van Dongen et. al](https://micans.org/mcl/)). 

As a first step, we will construct an adjacency matrix from a similarity matrix using the function `build_adjacency_matrix`. This function takes as input either a pre-computed similarity matrix or the raw counts from which to calculate a correlation matrix. In addition, it requires a value for the similarity cutoff. It then builds a binary matrix of 1s and 0s form this information. A 1 means the similarity was larger than the cutoff. We can set the  cutoff to "auto", this way, the algorithm will look for a valley in the distribution of similarities and use this as a cutoff. The function returns a list with the following entries:

* adj: The adjacency matrix
* cor: The similarity matrix
* cutoff: The similarity cutoff

We will use the 1 -  the relative euclidean distance in PCA space as our similarity measure:

```{r, message = F}
sim = 1-as.matrix(dist_eucl)/max(dist_eucl)
adj_corr = build_adjacency_matrix(mat = sim,cutoff="auto",is_similarity=T)
```

Now we run MCL with the above list as input:

```{r, message = F}
# use R MCL:
groups_MCL = MCLcell.clust(adj_corr, use_external_mcl = FALSE)
#use external MCL (UNIX only)
#groups_MCL = MCLcell.clust(adj_corr, use_external_mcl = TRUE, mcl_path = mcl_path)
sce_info$MCL = as.factor(groups_MCL)
table(sce_info$MCL,sce_info$hclust_eucl)
```

We see that the MCL found only three clusters, merging the B and T cells:

```{r, fig.width = 8}
p = my_plot_tSNE(tsne = tsne_info, color = colData(sce_info)[,"MCL",drop=F],
                 shape = colData(sce_info)[,"hclust_eucl",drop=F],
                 title = "MCL (cutoff=auto) assignment",
                 show_proportions = T)
print(p)
```

We can try changing the cutoff for the calculation of the adjacency matrix and re-run the algorithm:

```{r,fig.height=6,fig.width = 9}
adj_corr = build_adjacency_matrix(mat = sim,cutoff=0.8,is_similarity=T)
groups_MCL = MCLcell.clust(adj_corr,mcl_path = mcl_path)
sce_info$MCL2 = as.factor(groups_MCL)
table(sce_info$MCL2,sce_info$hclust_eucl)

p = my_plot_tSNE(tsne = tsne_info, color = colData(sce_info)[,"MCL2",drop=F],
                 shape = colData(sce_info)[,"hclust_eucl",drop=F],
                 title = "MCL (cutoff=0.8) assignment",
                 show_proportions = T)
p = p + scale_color_manual(values = brewer.pal(10,"Paired"))
print(p)
```

We note that by raising the similarity cutoff, we can force MCL to make more clusters. We now find the expected populations. 

In conclusion, MCL performs similar to hierearchical clustering. Its main advantage is that it is much faster than hclust and can therefore be used on datasets with ten thousands of cells. The main drawback is that MCL is very sensitive to the provided input. Using a similarity measure that does not appropriately separate cells or using the wrong similarity cutoff will lead to MCL making lots of clusters with just one cell inside. The automated cutoff determination should provide some guidance here, but it is not guaranteed to work in all cases. 


### Density clustering (DBSCAN)
DBSCAN is a density clustering algorithm that is based on a k-nearest neighbor search. In a nutshell, it identifies clusters as regions in space that contain many points with shared neighbors. DBSCAN works best in a lower dimensional space, so we weill again use the coordinates in PCA space as input. The `run_dbscan` function requires the following inputs:

* a distance matrix
* minPts: The minimal number of neighbors a point has to have to be considered "high density". Usually, this is set to the dimensionality of the dataset +1, so here, we set it to the number of principal components used (6) + 1.
* eps: Size of the epsilon neighborhoos. If we make a k-nearest neighbor distance plot using `kNNdistplot`, eps is the y-value at the "elbow" of that plot. For convenience, we can set eps to "auto", then the function `run_dbscan` will automatically set it.
* tol (optional): This is a parameter for the tolerance when determining eps. The higher it is, the higher the returned eps will be. We set it to 0.005 here, this way, eps is exactly at the elbow of the kNNdistplot.

```{r}
DBSCAN_groups = run_dbscan(dist_eucl,eps="auto",min_pts=7,tol=0.02)
```

Visualize the assignment:

```{r,fig.height=6,fig.width=8}
sce_info$DBSCAN = as.factor(DBSCAN_groups)
p = my_plot_tSNE(tsne = tsne_info, color = colData(sce_info)[,"DBSCAN",drop=F],
                 shape = colData(sce_info)[,"hclust_eucl",drop=F],
                 title = "DBSCAN assignment",show_proportions = T)
print(p)

```

We see that DBSCAN correctly identified the clearly separated clusters, but failed for the more continuous ones. This illustrates the main disadvantage of this algorithm: It can only find clusters that have no density between them. On the bright side, it is really fast and it does not require any assumptions of the cluster shape or size.

Save the SCESet that now contains all the assignments:

```{r, eval = T}
save(sce_info,file = file.path(out_data_dir,"sce_info.RData"))
```

## Differential expression analysis
The workflow contains wrapper functions for running differential expression analysis using several widely-used packages. For a comparison of methods, consider the recent study by Soneson and Robinson, which can be found [here](https://doi.org/10.1101/143289). 

The main findings of the study are that methods differ in the number and characteristics of the genes they report as differentially expressed in a manner that is dependent on the respective dataset. Performance of the methods overall was found to be similar across methods, with those developed for bulk data not being significantly worse than those specifically designed for single-cell data.

If we are interested in finding a small number of high-confidence marker genes, it might be most useful to run several methods and check what their overlap is. In general, I found the methods to be fairly consistent, especially for high expressed genes. For low expressed genes, results from any analysis should be taken with a grain of salt because of the very high underlying technical variability. Special care should be taken when comparing cell types that differ in size and/or RNA content and therefore have cell-specific dropout rates, in which case most low expressed genes are only detected in the large cells.

Currently, wrappers to one non-parametric test (Wilcoxon/Mann-Whitney) and two methods based on linear models (limma, MAST) are provided.

Every wrapper function takes the same four variables as input:

* sce: the full SCESet after QC and normalization
* cl_id: The name of the grouping that should be used. Here, we use the assignment obtained from hiererchical clustering on distances in PCA space, which we called "hclust_eucl".
* cl_ref: Which cluster we want to compare against all the others. We select "B-cell". Note: If you wish to do a direct comparison between two clusters, you can do so by setting all assignments but the two you want to compare to `NA`, which for factors is very convenient using `droplevels()`.
* fc_cutoff and alpha: The cutoffs for fold changes and p-values used to determine whether a gene is significantly DE. Note that your cutoffs have no effect on the analysis, and because the results for all genes are returned, you can still change the cutoffs afterwards.

Before we can start, we need to add some of the cluster assignments we obtained in the previous section to the SCESet that contains all genes:

```{r}
sce_clean$SC3_assignment = sce_info$SC3_assignment
sce_clean$hclust_eucl = sce_info$hclust_eucl
```

### Wilcoxon test
We can run the Wilcoxon test using the `run_wilcoxon_test` wrapper function. This will take as input the above specified variables and return a `data.table` containing (adjusted) p-values and fold changes per gene.

NOTE: The `pseudocount` parameter is deprecated and will be removed!

```{r}
de_wilcox = run_wilcoxon_test(sce_clean, cl_id = "hclust_eucl", cl_ref = "B-cell",
                           fc_cutoff = 1, alpha = 0.05)
dim(de_wilcox)
head(de_wilcox)
```

We see that the function returned a `data.table` containing all gene IDs plus the test statistics and estimated fold changes for each of them. We can use the DE_flag entry to check how many genes were found to be significantly DE (i.e. adjusted p-value < 0.05 and absolute log2 fold change > 1).

```{r}
table(de_wilcox$DE_flag)
```

A useful way to check whether the method was biased towards calling genes in a specific expression range DE is to plot log2 fold changes v.s. the mean expression of a gene. We will add the mean_exprs column in the feature metadata to the `de_wilcox` table by using `merge` from the `data.table` package:

```{r}
mean_exprs_dt = data.table(gene_id = rownames(sce_clean),mean_exprs = rowMeans(norm_exprs(sce_clean)))
de_wilcox = merge(de_wilcox,mean_exprs_dt, by = "gene_id")
names(de_wilcox)
```

We note that the mean_exprs column has been added to `de_wilcox`. Now we make a plot by calling `generic_scatterplot`. This function takes a `data.table` as input and makes a scatterplot of two columns, x_col and y_col, against each other. Optional column names can be provided to control color, shape and size of the points. The function returns a `ggplot` object, to which we can easily add other features, for example, a title:

```{r}
p = generic_scatterplot(de_wilcox, x_col = "mean_exprs", y_col = "log2fc",
                        color = "DE_flag")
print(p+ggtitle('MA plot for Wilcoxon test'))
```

We see that because fold changes are estimated as the difference in log2 mean expression rather than a ratio, very low expressed genes have fold changes around zero.

To make a volcano plot, we can again use `generic_scatterplot`:
```{r}
de_wilcox[,log10_pval:=-log10(adj_pval)]
p = generic_scatterplot(de_wilcox, x_col = "log2fc", y_col = "log10_pval",
                        color = "DE_flag")
print(p+ggtitle('Volcano plot for Wilcoxon test'))
```

Note: In general, the more cells you have, the more tiny changes will become significant. The `DE_flag` parameter is therfore controlled mostly by your fold change cutoff, unless you set a very stringent p-value cutoff (~10^-20). It is probably best to use the fold change / p-value combination more as a way of ranking genes than as an absolute threshold and focus on the genes that come out on top.

Now let's check whether the identified genes make sense:

```{r,message=F}
library(DT)
top_DE = de_wilcox[DE_flag==TRUE][order(log2fc,decreasing = T)]$gene_id[1:20]
gene_table = get_gene_annotations(top_DE,get_descriptions = T)
datatable(gene_table, caption = "Top 20 upregulated genes in B-cells")
```

And we are happy to see that the top upregulated genes correspond to known B-cell markers such as CD79 or MHCII subunits.

### limma
Limma was originally developed for bulk RNA-seq data, but in most cases, it also performs fairly well on single-cell data. However, especially the voom version can become unreliable in the presence of many zero counts, therefore, it is recommended to filter out low-abundant genes prior to running the analysis. In the following, we will run both limma-voom and limma-trend and compare their outputs.

The function `run_limma` is used for both methods. In addition to the standard input described above, we need to provide the following inputs:

* method: which limma method we want to use. Trend is more stable than voom. 
* count_thr, pct: Threshold for gene filtering. Genes that do not have a minimum of `count_thr` counts in at least `pct` percent of cells in at least one cluster will be ignored. For this extremely sparse data, we set this to 1.

```{r}
de_limma_trend = run_limma(sce_clean, cl_id = "hclust_eucl", cl_ref = "B-cell",
                          method = "trend", fc_cutoff = 1, alpha = 0.05, count_thr = 1, pct=50)

de_limma_trend = merge(de_limma_trend, mean_exprs_dt)

p = generic_scatterplot(de_limma_trend, x_col = "mean_exprs", y_col = "logFC",
                        color = "DE_flag")

print(p + ggtitle("MA plot for limma-trend"))

```

Limma-trend finds exactly the same genes as the wilcoxon test, probably because the way fold changes are estimated is the same and we filter mainly based on fold changes. Comparing the p-values, limma-trend and the Wilcoxon test still agree very well.



### Using SC3 to find markers (broken currently)

This sections shows how to tweak the `sc3_calc_biology` function to work with any arbitrary clustering.

```{r, eval = F, message = F}
library(SC3)

# add the slots the calc biology function checks
dummy = list(`0`=0)
sce_clean@metadata$sc3$consensus = dummy
sce_clean@metadata$sc3$n_cores = 4
rowData(sce_clean)$sc3_gene_filter = rep(TRUE,dim(sce_clean)[1]) #do not filter out any genes
rowData(sce_clean)$feature_symbol = rowData(sce_clean)$symbol

# add whatever clustering assignment you want to the sc3_0_clusters slot
sce_clean$sc3_0_clusters = as.factor(sce_info$cell_type) 

sce_clean = sc3_calc_biology(sce_clean, k = 0)
```

Plot markers:

```{r, out.width="110%",fig.height=6, eval=F}
# change the plotted gene names to symbol for better readability
plot_sce = sce_clean[!is.na(rowData(sce_clean)$symbol),]
rownames(plot_sce) = rowData(plot_sce)$symbol
custom_sc3_plot_markers(plot_sce, k=0, p.val = 0.01, auroc = 0.90, show_pdata='sc3_0_clusters')
```

## Session Info
```{r}
sessionInfo()
```

## Functions
This section lists all custom functions that are part of the workflow. These are equivalent to everything that is provided in the ./code directory, therefore this section is merely for documentation purposes.  

### Setup

#### library_calls()

The function _library_calls()_ calls all the libraries that are required throughout the workflow. Additional packages are loaded whenever needed. The reason for this is that some of the newer bioconductor packages load a million dependencies, and currently, R has a limit on how many packages can be loaded at any time.

#### get_gene_annotation()

This is the default function to annotate genes. It uses the ensembldb and EnsDb.Hsapiens.v79 packages. Ensembldb is preferred over biomaRt because it is much faster and it uses a specific annotation package, making it more reproducible. Optionally, gene descriptions in human-readable form are obtained from org.Hs.eg.db.

Input:

* gene_list = a list of ensembl gene IDs as a character vector
* get_descriptions = should descriptions be obtained from org.Hs.eg.db/org.Mm.eg.db? TRUE or FALSE. Note that this is slow for a long gene list.
* v = verbose flag. If set to true, prints all the errors that my_get() catches. Should be set to true only for debugging purposes.
* organism: one of either "mouse" or "human"

Output: 

* geneName = a data.table of gene annotations. Note that this table is sorted by gene identifier.

#### get_gene_annotations_biomart()
Gene annotations can also be retrieved from biomaRt, however, ensembldb is preferred for the above mentioned reasons.

### Quality control

#### Filtering of genes and cells
The functions below are used for manual filtering of cells and genes based on several QC metrics. 

Input:

* counts: Matrix of raw counts.
* n_th: A single number specifying the cutoff for the respective filter. In the gene filter, this is the minimal number of cells with counts above `min_counts`. In the feature_counts and n_UMI filters, this is the minimal number of total counts or total UMIs, respectively, per cell.
* min_counts: In the gene filter, this specifies the minimum number of counts. The filter removes genes that do not have at least `min_counts` counts in at least `n_th` cells.
* mt_genes.amount: a vector containing the percentage of mitochondrial genes per cell. This information is stored in sce$percent_feature_controls_MT.
* t: a single number specifying the maximum percentage of reads mapped to mitochondrial genes.


#### Visualizing the manual filters
Input:

* sce, input_sce: an SCESet from which the things to plot are taken
* min_genes, min_UMI, t: the cutoffs selected for minimum number of genes per cell [log2], minimum number of UMIs per cell [log2], and maximum percentage of mitochondrial genes

#### Cell cycle assignment

Using the cyclone function from scran:

```{r}
annotate_cell_cycle = function(sce, organism = "human", gene.names = rownames(sce)){
  if(organism == "human"){
    hs.pairs = readRDS(system.file("exdata", "human_cycle_markers.rds", package="scran"))
    assigned = cyclone(sce, pairs=hs.pairs, gene.names = gene.names)} else if (organism == "mouse"){
      mm.pairs = readRDS(system.file("exdata", "mouse_cycle_markers.rds", package="scran"))
      assigned = cyclone(sce, pairs=mm.pairs, gene.names = gene.names)
    } else {stop("Organism has to be human or mouse.")}
  
  return(assigned)
}
```


### Normalization
All available normalizations are collapsed in a single function `normalize_counts` for convenience. 

Input:

  * sce: SCESet. Contains all your data.
  * method: Character. The method you want to use to normalize. Choices are:
    + TC = total count normalization (multiply this by 10^6 to get CPMs)
    + UQ = upperquartile
    + RLE = relative log-expression, as in DESeq2
    + TMM = trimmed mean of M-values, as in edgeR
    + scran (default) = Lun sum factors, implemented in scran package

Output:

 * an SCESet with the normalized expression values in the exprs and norm_exprs slots 
 

### Feature selection

#### Highly variable genes (Brennecke et. al, 2013)
This method implements the approach presented by Brennecke et. al, NMeth, 2013 ([here](http://www.nature.com/nmeth/journal/v10/n11/full/nmeth.2645.html?foxtrotcallback=true)).

Input:

* x: a matrix of normalized counts, NOT on log scale. Note that therefore you will need to provide 2^norm_exprs(SCESet)-1 if your input comes from an SCESet.
* PLOT: logical. Should a plot be printed?
* qcv: Between 0 and 1. Threshold for the coefficient of variation (see below). 
* q: Between 0 and 1. The q-th quantile of the mean expression of all genes having a CV > qcv is used as a threshold. Genes having mean below this threshold are not included in the analysis.
* pv: the p-value cutoff
* minBiolDisp: Between 0 and 1. The CV of the underlying biological variation. Leave at default unless you have a good reason to change it.

Output:

 * info: a vector of logicals specifying whther a gene is informative or not. 
 

#### Using depth-adjusted negative binomial model (M3Drop)
This method is implemented in M3drop versions > 2.0. It models the gene counts as a negative binomial with a depth-adjusted mean. Gene-specific dispersions are then fit to the sample variance. A detailed description of the method can be found [here](https://www.biorxiv.org/content/early/2016/10/20/065094). Based on the calculated model, informative genes can be selcted by comparing expected values for dropouts (NBDrop) or dispersions (NBDisp) to the predicted ones.

The function _run_DANB_ is used for both methods.

Input:

* counts: a matrix of RAW counts
* save_plot: logical. Should the produced plot be saved?
* method: One of either "NBDrop" or "NBDisp". The method used for feature selection.
* cutoff: The p-value cutoff for NBDrop or the percentage of genes returned by NBDisp.
* perc_genes: The percentage of genes to keep (can only be provided instead of cutoff)

Output:

*  info: a vector of logicals specifying whther a gene is informative or not. 

#### Select genes based on GO annotation

Input:

* go_id: A GO identifier
* organism: One of either "human" or "mouse"

Output:

* gene_ens: a list of ensembl gene identifiers mapping to this GO term.


### Clustering
#### MCL (adapted from code provided by Marilisa Neri)
Note: This method requires an external installation of MCL which can be obtained from [here](http://micans.org/mcl/). It consists of two steps: First, _build_adjacency_matrix_ calculates an adjacency matrix from some measure of similarity (currently, the only possibility is pearson correlation but I will probably change this in the future). Second, the adjacency matrix is converted to a graph and used as input for MCL. Note that MCL is extremely sensitive to the cutoffs that are chosenn to construct the adjacency matrix. As a rule of thumb, I look for the valley in the distribution of similarities and use this as a cutoff. If this does not work (i.e. MCL returns lot of tiny clusters), consider specifying the cutoff manually.

Input:

 * mat: a matrix of normalized expression values on a log2 scale or a matrix of similarities if is_similarity = TRUE
 * cutoff: Either "auto" or a number between 0 and 1. Parameter for the correlation cutoff used to contruct the adjacency matrix. If set to "auto", the function will attempt to find a valley in the distribution of similarities and use this as a cutoff. If a number is provided, it will use this as a cutoff.
 * adj: List. The output of _build_adjacency_matrix_, a list containing the adjacency matrix, the similarity matrix and the chosen cutoff.
 
Output: 

* groups.MCL: The cluster assignments. "0" means the cell was not assigned to any cluster.

#### DBSCAN
DBSCAN is a density based algorithm that identifies clusters as regions of high density that are separated by regions of low density. Note that it is not suited for very high-dimensional data, therefore should be used only in combination with dimensionality reduction (usually PCA).

Input:

* dist: A dist object containing cell-cell distances in low-dimensional space.
* min_pts: Integer. The number minimum points in the eps neighborhood. Generally, this should be one more than the dimensionality of the space in which distances were calculated, i.e. the number of used principal components +1. 
* eps: "auto" or a number. The radius of the eps neighborhood (see dbscan documentation for details). As a rule of thumb, this should be the y value of the "elbow" on the KNNdistplot. If set to "auto", eps will be determined automatically.
* tol: The tolerance when determining eps. The default is 0.01, which in general works quite well. The lower the tolerance, the smaller eps.
  
### Differential Expression Analysis
The functions in the following are wrappers to the respective packages. for details what each of them does, please refer to the package documenatations.

General input for all of them:

* sce = the SCESet after QC or after normalization
* cl_id : the name of the grouping (e.g. "mclust") used in the comparison. Must be
  a column name of colData(sce)
* cl_ref: the entry in colData(sce)$cl_id that should be used as the reference,
 i.e. against which all the rest of the cells are compared
 * alpha = false discovery rate cutoff, default = 0.05
* fc_cutoff = log2 fold change cutoff, default = 0.5

#### Wilcoxon test
This function runs the Wilcoxon test to calculate p-values, adjusts them for multiple correction using Benjamini-Hochberg method, and calculates fold changes. There is one additional parameter:

* pseudocount: Deprecated and will be removed!


#### limma
This function implements the limma-voom and limma-trend methods. It takes three additional parameters:

  * count_thr, pct: Filter criteria. Genes that do not have a minimum count of `count_thr` in `pct`% of cells in at least one cluster are excluded. This is required because limma can become unreliable in the presence of too many very low expressed genes.
  * method: one of "voom" or "trend". Specifies which limma method to use. The default is "trend" since this seems to work better with zero-inflated data.
  
### Miscellaneous Plotting

#### Generic scatterplot using ggplot2 and data.table

This function takes a data.table as input and plots two columns, x_col and y_col, against each other. Optional arguments can be provided to control color, size and shape of the plotted symbols. 

Input:

* dt:  data.table with all the data to be plotted
* x_col: name of the column to lot on x axis
* y_col: name of column to plot on y axis
* color: name of column to use as color values. If numeric, a continuous color scheme will be applied, if character or factor, colors will be discrete.
* shape: name of column to use for shape
* size: name of column to use for size
* alpha: transparency
* abs_size; If no values for size are provided, abs_size sets the absolute size of the points on the plot.

Output:

* p: The plot as a ggplot object

#### PCA plot
Calculates a PCA and makes a plot.

Input:

* counts: A matrix of log2 transformed, raw or normalized counts. If no counts are provided, a pre-computed prcomp object has to be provided instead.
* pca: prcomp object. A result from a previous PCA calculation. If missing, a new PCA is calculated.
* scale_pca: Logical. Should the data be scaled to have unit variance? Default = TRUE.
* center_pca: Logical. Should the data be centered before calculating PCA? Default = TRUE.
* comp: Numerical vector with 2 elements. The 2 components to plot against each other.
* return_pca: Logical. Should the prcomp object be returned? If TRUE, the function returns a list with pca = the prcomp object and plot = the plot. If false, only the plot is returned as a ggplot2 object.
* use_irlba: Logical. Should the principal components be calculated using prcomp_irlba? Using irlba ignificantly increases speed for large datasets.
* color, size and shape: Each a data.frame with one column containing the respective values to plot. For colors, numeric values are plotted on a continuous scale, factors as discrete colors. Note that the rownames of the data.frame have to  be the same as the rownames of pca, but can be in any order. By default, the column name is used as a label for the color legend. The easiest way of setting any of those values is to use columns in the phenoData of your SCESet, e.g. colors = colData(sce)[,"cluster",drop=F]. 
* alpha: Between 0 and 1. Transparency of the points.
* tite: Title of the plot.

#### tSNE plot
Pretty much the same as the PCA plot function, but makes a tSNE map instead.

Input:

* counts: A matrix of log2 transformed, raw or normalized counts or a distance matrix. If a distance is used, is_distance has to be set to TRUE. If no values are provided, a pre-computed Rtsne object has to be provided instead.
* tsne: Rtsne object. A result from a previous tSNE calculation. If missing, a new tSNE is calculated.
* return_tsne: Logical. Should the Rtsne object be returned? If TRUE, the function returns a list with tsne = the Rtsne object and plot = the plot. If false, only the plot is returned as a ggplot2 object.
* is_distance: Logical. Is the input for Rtsne a distance matrix?
* scale_pca: Logical. Should the data be scaled to have unit variance? Default = FALSE.
* n_comp: Number of principal components used by tSNE.
* color, size and shape: Each a data.frame with one column containing the respective values to plot. For colors, numeric values are plotted on a continuous scale, factors as discrete colors. Note that the rownames of the data.frame have to  be the same as the rownames of pca, but can be in any order. By default, the column name is used as a label for the color legend. The easiest way of setting any of those values is to use columns in the phenoData of your SCESet, e.g. colors = colData(sce)[,"cluster",drop=F]. 
* alpha: Between 0 and 1. Transparency of the plotted points.
* tite: Title of the plot.
* show_proportions: Logical. if the plot is colored by a dsicrete variable, should the proprtions of points per color be shown in the legend?
